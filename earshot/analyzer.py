import tensorflow as tf, numpy as np, pandas as pd, _pickle as pickle
import os, glob

from .phonology import are_cohorts, are_neighbors, are_rhymes
from .progressbar import progress

class Analyzer:
    def __init__(self,analyzer_params,pattern_parameters):
        '''
        Class for taking checkpointed output of earshot and calculating model accuracy
        (as a function of epoch) and time-dependent accuracy, over pattern (model internal)
        time for all the checkpointed model output.
        '''
        self.analyzer_patterns = analyzer_patterns
        self.pattern_parametes = pattern_parameters
        # load/accumulate pattern information
        self.load_pattern_metadata()
        self.generate_category_dict()
        self.generate_adjusted_length_dict()
        # calculate accuracies, category (target/cohort/rhyme/unrelated)
        self.analyze()
        # these parse the files generated by self.analyze() and produce pickles
        #   with the summary data
        self.parse_rt_file()
        self.parse_cf_file()


    def load_pattern_metadata(self):
        '''
        Loads metadata info from pre-generated patterns made by patterns.PatternGenerator.
        '''
        with open(os.path.join(self.pattern_parameters.pattern_path, self.pattern_parameters.metadata_file).replace('\\', '/'), 'rb') as f:
            self.pattern_metadata = pickle.load(f)

        self.word_index = {word:index for index, (word,_) in enumerate(self.pattern_metadata['Target_Dict'].items())}
        self.step = {self.pattern_metadata['Word_and_Identifier_Dict'][path]: step for path,step in self.pattern_metadata['Step_Dict'].items()}
        self.max_step = max([step for step in self.step.values()])
        self.targets = np.array([self.pattern_metadata['Target_Dict'][word] for word, _ in sorted(list(self.word_index.items()), key = lambda x: x[1])]).astype(float32)


    def generate_category_dict(self):
        '''
        For each word, determines its cohorts, rhymes, DAS neighbors, and unrelated words from the
        pronunciation file.
        '''
        self.category_dict = {}
        for target_word,target_pron in self.pattern_metadata['Pronounciation_Dict'].items():
            self.category_dict[target_word,'Target'] = []
            self.category_dict[target_word,'Cohort'] = []
            self.category_dict[target_word,'Rhyme'] = []
            self.category_dict[target_word,'DAS_Neighborhood'] = []
            self.category_dict[target_word,'Unrelated'] = []

            for compare_word, compare_pron in self.pattern_metadata['Pronounciation_Dict'].items():
                compare_word_indx = self.word_index[compare_word]

                # assume word is unrelated by default
                if target_word == compare_word:
                    # word is the target; move on
                    self.category_dict[target_word, 'Target'].append(compare_word_indx)
                    continue

                # words cannot be both cohorts and rhymes
                if are_cohorts(target_pron, compare_pron):
                    # words are in the same cohort
                    self.category_dict[target_word, 'Cohort'].append(compare_word_indx)
                    # they may also be neighbors
                    if are_neighbors(target_pron,compare_pron):
                        self.category_dict[target_word, 'DAS_Neighborhood'].append(compare_word_indx)
                    continue
                elif are_rhymes(target_pron, compare_pron):
                    # words are rhymes and (by this def'n) automatically neighbors
                    self.category_dict[target_word, 'Rhyme'].append(compare_word_indx)
                    self.category_dict[target_word, 'DAS_Neighborhood'].append(compare_word_indx)
                    continue

                # words may be neighbors but not cohorts or rhymes
                if are_neighbors(target_pron,compare_pron):
                    self.category_dict[target_word, 'DAS_Neighborhood'].append(compare_word_indx)
                    continue

                # if we made it here, they must be unrelated
                self.category_dict[target_word, 'Unrelated'].append(compare_word_indx)


    def generate_adjusted_length_dict(self):
        '''
        This is used to establish uniqueness point, according to Heejo.
        '''
        self.adj_length_dict = {}

        for word,pron in self.pattern_metadata['Prononciation_Dict'].items():
            for cut_len in range(1,len(pron)+1):
                cut_pron = pron[:cut_len]
                to_compare = [candidate[:cut_len] for candidate in self.pattern_metadata['Pronunciation_Dict'].values() if pron != candidate]
                if not cut_pron in to_compare:
                    self.adj_length_dict[word] = cut_len - len(pron) - 1
                    break
            if not word in self.adj_length_dict:
                self.adj_length_dict[word] = 0


    def analysis(self):
        '''
        Reads checkpointed .pickle files in the appropriate results directory (set in the analyzer_parameters) to
        generate run summary information files (competitort activation, accuracy as a function of epoch).  Currently, all
        files reside in a "Test" subdirectory and have the format E_#.I_#.pickle.  This are the only files that will
        be analyzed.

        The run summary information text files could be removed and the pickles written directly; when I wrote the parsers,
        I didn't really understand this function.  If we were sure the parsers harvest everything we want, we could remove
        the intermediate txt files. (K.B. 10/22/20)
        '''
        result_files = glob.glob(os.path.join(self.analyzer_parameters.model_output_path, 'Test','E_*.pickle').replace('\\', '/'))
        # dictionaries that will be read into files
        reaction_times = ['\t'.join(['{}'.format(x) for x in ['Epoch','Word','Identifier','Pattern_Type','Pronunciation','Pronunciation_Length','Uniqueness_Point',
            'Cohort_N','Rhyme_N','Neighborhood_N','Onset_Absolute_RT','Onset_Relative_RT','Onset_Time_Dependent_RT','Offset_Absolute_RT','Offset_Relative_RT','Offset_Time_Dependent_RT']])]

        category_flows = ['\t'.join(['{}'.format(x) for x in ['Epoch','Word','Identifier','Pattern_Type','Pronunciation','Pronunciation_Length','Uniqueness_Point',
            'Cohort_N','Rhyme_N','Neighborhood_N','Category','Category_Count','Accuracy'] + list(range(self.max_Step))])]

        for f in result_files:
            result_dict = pickle.load(open(f,'rb'))
            epoch = result_dict['Epoch']
            info = result_dict['Info']
            outputs = result_dict['Result']

            for index, (output, (word,identfier,pattern_type)) in enumerate(zip(outputs,info)):
                data = self.generate_data(output, word, identifier) #[Num_Words, Steps]
                rt_dict = self.generate_rt(word, identifier, data)
                cf_dict = self.generate_category_flow(word, data)
                reaction_times.append('\t'.join(['{}'.format(x) for x in [epoch,word,identifier,pattern_type,'.'.join(self.pattern_parameters.lexicon[word]),
                        len(self.pattern_parameters.lexicon[word]),self.adj_length_dict[word],len(self.category_dict[word, 'Cohort']),len(self.category_dict[word, 'Rhyme']),
                        len(self.category_dict[word, 'DAS_Neighborhood']),rt_dict['Onset', 'Absolute'],rt_Dict['Onset', 'Relative'],rt_Dict['Onset', 'Time_Dependent'],
                        rt_Dict['Offset', 'Absolute'],rt_Dict['Offset', 'Relative'],rt_Dict['Offset', 'Time_Dependent']]]))

                for category in ["Target", "Cohort", "Rhyme", "Unrelated", "Other_Max"]:
                    if category == "Other_Max":
                        category_count = np.nan
                    else:
                        category_count = len(self.category_dict[word, category])
                    category_flows.append('\t'.join(['{}'.format(x) for x in [epoch,word,identifier,pattern_type,'.'.join(self.pattern_parameters.lexicon[word]),
                        len(self.pattern_parameters.lexicon[word]),self.adj_length_dict[word],len(self.category_dict[word, 'Cohort']),len(self.category_dict[word, 'Rhyme']),
                        len(self.category_dict[word, 'DAS_Neighborhood']),category,category_count,not np.isnan(rt_dict["Onset", "Time_Dependent"])]
                        + ['{:.5f}'.format(x) for x in cf_dict[category]]]))
                # write the progress bar
                progress(index + 1,outputs.shape[0],status=f)
            # newline? flushes the buffer? not sure why this is here
            print()
        # these files could be removed if we were sure the parse_ functions harvest everything we want
        with open(os.path.join(self.analyzer_parameters.model_output_path, 'Test', 'RTs.txt').replace('\\', '/'), 'w') as f:
                f.write('\n'.join(reaction_Times))
        with open(os.path.join(self.analyzer_parameters.model_output_path, 'Test', 'Category_Flows.txt').replace('\\', '/'), 'w') as f:
                f.write('\n'.join(category_Flows))


    def generate_data(self, output, word, identifier):
        '''
        Heejo says: Data generation is progressed pattern by pattern, not multiple pattern because GPU consuming.
        output: [Steps, Dims]

        N.B. generate_data() is a terrible name for this function.  I think this just stacks up single calculations of
        cosine similarity from calculate_data? (K.B.)
        '''
        cs_list = []
        for batch_index in range(0, output.shape[1], self.analyzer_parameters.batch_step):
            cs_list.append(self.calculate_data(output= output[batch_index:batch_index + self.analyzer_parameters.batch_step]))
        cos_sim = np.hstack(cs_list)
        if self.analyzer_parameters.step_cut:
            cos_sim[:, self.step[word, identifier]:] = cos_sim[:, [self.step[word, identifier] - 1]]
        return cos_sim


    @tf.function
    def calculate_data(self,outputs):
        '''
        This actually computes cosines.

        Heejo says:
                output: [Steps, Dims]
                self.targets: [Num_Words, Dims]
        '''
        output = tf.convert_to_tensor(output, dtype=tf.float32)
        targets = tf.convert_to_tensor(self.targets, dtype=tf.float32)

        #Heejo: [Num_Words, Steps, Dims], increase dimension and tiled for 2D comparing.
        tiled_output = tf.tile(tf.expand_dims(output, [0]),multiples = [tf.shape(targets)[0], 1, 1])
        # Heejo: [Num_Words, Steps, Dims], increase dimension and tiled for 2D comparing.
        tiled_targets = tf.tile(tf.expand_dims(targets, [1]),multiples = [1, tf.shape(output)[0], 1])
        # do the calculation
        ttto = tiled_targets*tiled_output
        ttsq = tf.pow(tiled_targets,2)
        tosq = tf.pow(tiled_output,2)
        cos_sim = tf.reduce_sum(ttto, axis = 2)/(tf.sqrt(tf.reduce_sum(ttsq, axis = 2)) * tf.sqrt(tf.reduce_sum(tosq, axis = 2)) + 1e-7)  #[Num_Words, Steps]

        return cos_sim


    def generate_rt(self, word, identifier, data):
        '''
        Operationalizes accuracy using three different definitions and the cutoff parameters supplied
        in the analyzer options.

        absolute accuracy:
            point at which the most active word crosses the absolute activity threshold

        relative accuracy:
            point at which the activity of the most active word exceeds the next most active word by the
            relative threshold

        time-dependent accuracy:
            activity of the most active word has to be greater than the activity of the next most
            active word by a threshold and maintain that difference for a given number of time steps
        '''
        rt_dict = {('Onset', 'Absolute'): np.nan,('Onset', 'Relative'): np.nan,('Onset', 'Time_Dependent'): np.nan}

        target_index = self.word_index[word]
        target_array = data[target_index]
        # this is the runner up word (after the target)
        other_max_array = np.max(np.delete(data, target_index, 0), axis=0)

        # makes for less typing
        abs_cut = self.analyzer_parameters.abs_acc_crit
        rel_delta = self.analyzer_parameters.rel_acc_crit
        td_cut = self.analyzer_parameters.td_acc_crit[1]
        td_T = self.analyzer_parameters.td_acc_crit[0]

        # absolute RT
        if not (other_max_array > abs_cut).any():
            abs_check_array = target_array > abs_cut
            for step in range(self.max_step):
                if abs_check_array[step]:
                    rt_dict['Onset', 'Absolute'] = step
                    break
        # Calculate Offset RT
        if not np.isnan(rt_dict['Onset', 'Absolute']):
            rt_dict['Offset', 'Absolute'] = rt_dict['Onset', 'Absolute'] - self.step[word, identifier]
        else:
            rt_dict['Offset', 'Absolute'] = np.nan

        #relative RT
        rel_check_array = target_array > (other_max_array + rel_delta)
        for step in range(self.max_step):
            if relative_check_array[step]:
                rt_dict['Onset', 'Relative'] = step
                break
        # Calculate Offset RT
        if not np.isnan(rt_dict['Onset', 'Relative']):
            rt_dict['Offset', 'Relative'] = rt_dict['Onset', 'Relative'] - self.step[word, identifier]
        else:
            rt_dict['Offset', 'Relative'] = np.nan

        # time-dependent RT
        td_check_array_crit = target_array > other_max_array + td_cut
        td_check_array_sus = target_array > other_max_array
        for step in range(self.max_step - td_T):
            if all(np.hstack([td_check_array_crit[step:step + td_T],td_check_array_sus[step + td_T:]])):
                rt_Dict['Onset', 'Time_Dependent'] = step
                break
        # Calculate Offset RT
        if not np.isnan(rt_dict['Onset', 'Time_Dependent']):
            rt_dict['Offset', 'Time_Dependent'] = rt_dict['Onset', 'Time_Dependent'] - self.step[word, identifier]
        else:
            rt_dict['Offset', 'Time_Dependent'] = np.nan

        return rt_dict


    def generate_cf(self, word, data):
        '''
        Over-time similarity of target words to cohorts and rhymes, as well as unrelated words
        '''
        cf_dict = {}

        for category in ['Target', 'Cohort', 'Rhyme', 'Unrelated']:
            if len(self.category_dict[word, category]) > 0:
                # mean over several flows from the same category
                cf_dict[category] = np.mean(data[self.category_dict[word, category],:], axis=0)
            else:
                # nan if the word doesn't match any category
                cf_dict[category] = np.zeros((data.shape[1])) * np.nan

        cf_dict['All'] = np.mean(data, axis=0)
        # most activated word other than the target
        cf_dict['Other_Max'] = np.max(np.delete(data, self.word_index[word], 0), axis=0)

        return cf_dict


    def parse_rt_file(self):
        '''
        Reads the "RTs.txt" file produced by EARShot and creates an Epoch-Accuracy
        dictionary for onset- and off-set relative, absolute, and time-dependent
        accuracies. Saves the result to a pickle.
        '''
        acc_file = os.path.join(self.analyzer_patterns.model_output_path, 'Test', 'RTs.txt').replace('\\', '/')
        data = pd.read_csv(acc_file,sep='\t')
        # dictionary to hold the results
        acc_data = {}.fromkeys(data.columns[10:].values)
        epochs = list(np.unique(data["Epoch"]))
        for meas in acc_data:
            acc_data[meas] = {}.fromkeys(np.unique(data["Epoch"]))

        for e in epochs:
            subframe = data[data["Epoch"] == e]
            for meas in acc_data:
                n_wrong = subframe[meas].isnull().sum()
                n_total = len(subframe[meas])
                acc_data[meas][e] = (n_total - n_wrong)/n_total

        # dump the results
        fname = os.path.join(self.result_Path, 'Test', 'ACC.pydb').replace('\\', '/')
        pickle.dump(acc_data,open(fname,'wb'))


    def parse_cf_file(self):
        '''
        Reads the "Category_Flows.txt" file produced by EARShot and creates average
        similarities over (recurrent model) time to target, cohort, rhymes, and
        unrelated words in the lexicon, for each checkpointed epoch.

        I'm not handling speakers/word/etc. at all right now - this just
        averages over every correct target response in every epoch.

        Saves the resulting dictionary to a pickle.
        '''
        cf_file = os.path.join(self.analyzer_patterns.model_output_path, 'Test', 'Category_Flows.txt').replace('\\', '/')
        data = pd.read_csv(cf_file,sep='\t')

        # set up the dictionary to hold the results
        cat_data = {}.fromkeys(np.unique(data["Epoch"]))
        # there's nothing useful in the zero-epoch set
        cat_data.pop(0)
        for e in cat_data:
            cat_data[e] = {}.fromkeys(np.unique(data["Category"]))

        # now accumulate stats for all epochs > 0; ignore any incorrect responses
        for e in cat_data:
            epoch = data[(data["Epoch"] == e) & (data["Accuracy"] == True)]
            # appropriate selections
            for c in cat_data[e]:
                mean_series = epoch[epoch['Category'] == c].mean()
                cat_data[e][c] = mean_series.values[13:]

        # now dump the results
        fname = os.path.join(self.result_Path, 'Test', 'CS.pydb').replace('\\', '/')
        pickle.dump(cat_data,open('fname','wb'))
